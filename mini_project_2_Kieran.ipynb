{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5f64d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \n",
    "     def forward(self,*input):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "     def backward(self,*gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "     def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3868d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \n",
    "    def __init__(self, modules):\n",
    "        self.modules = modules\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        output = self.input\n",
    "        \n",
    "        for module in self.modules:\n",
    "            output = module.forward(output)\n",
    "        return output\n",
    "    \n",
    "    def backward(self,*gradwrtouput):\n",
    "        gradient = gradwrtouput\n",
    "        for module in reversed(self.modules):\n",
    "            gradient = module.backward(gradient)\n",
    "        self.input = None\n",
    "        return gradient\n",
    "    \n",
    "    def param(self):\n",
    "        params = []\n",
    "        for module in self.modules:\n",
    "            params.append(module.param())\n",
    "        return params\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a44146f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def step(self):\n",
    "        return NotImplementedError\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        return NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94661872",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    \n",
    "    def __init__(self,params,lr, mu = 0, tau = 0):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        \n",
    "        # parameters in order to add momemtum \n",
    "        self.momemtum = mu\n",
    "        self.dampening = tau\n",
    "        self.state_momemtum = None\n",
    "        \n",
    "    def step(self):\n",
    "        for x, grad in self.params:\n",
    "            x.add_(-self.lr * grad) \n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for x, grad in self.params:\n",
    "            grad = grad.zero_()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "913048a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Module):\n",
    "    \n",
    "    def forward(self,input,target):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        return (self.input - self.target).pow(2).mean() \n",
    "    \n",
    "    def backward(self):\n",
    "        return 2*(self.input - self.target).div(torch.tensor(self.input.size(0)))\n",
    "        ## we divide by the batch size as in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c22c5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9983)\n",
      "tensor(1.9983)\n",
      "torch.Size([100, 3, 10])\n"
     ]
    }
   ],
   "source": [
    "#Check MSE loss function\n",
    "mse = MSE()\n",
    "input = torch.normal(mean=torch.zeros(100,3,10),std = 1)\n",
    "target = torch.normal(mean=torch.zeros(100,3,10),std = 1)\n",
    "loss = mse.forward(input,target)\n",
    "print(loss)\n",
    "mse_ = torch.nn.MSELoss()\n",
    "loss_ = mse_.forward(input,target)\n",
    "print(loss_)\n",
    "\n",
    "print(mse.backward().size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "668203f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    \n",
    "    def forward(self,input):\n",
    "        self.input = input\n",
    "        self.sigmoid = 1./(1+(-self.input).exp())\n",
    "        return  self.sigmoid\n",
    "    \n",
    "    def backward(self,*gradwrtouput):\n",
    "        return gradwrtouput*self.sigmoid*(1-self.sigmoid)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "088fd3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "input = torch.normal(mean=torch.zeros(100,3,38,38),std = 1)\n",
    "\n",
    "sig = Sigmoid()\n",
    "sig_ = torch.nn.Sigmoid()\n",
    "\n",
    "print(torch.allclose(sig.forward(input),sig_.forward(input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bb20769",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return (self.input>0.)*self.input\n",
    "    \n",
    "    def backward(self, *gradwrtouput):\n",
    "        return gradwrtouput*(self.input>=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd9e7c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "input = torch.normal(mean=torch.zeros(100,3,380,380),std = 1)\n",
    "\n",
    "re = ReLU()\n",
    "re_ = torch.nn.ReLU()\n",
    "\n",
    "print(torch.allclose(sig.forward(input),sig_.forward(input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a734070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5d86f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_upsampling(input, scale_factor):\n",
    "    if isinstance(scale_factor,int):\n",
    "        scale1 ,scale2 = scale_factor,scale_factor\n",
    "    if isinstance(scale_factor,tuple):\n",
    "        scale1 ,scale2 = scale_factor[0],scale_factor[1]\n",
    "    \n",
    "    N, C, H, W = tuple(input.size())\n",
    "    output = torch.empty(N,C,scale1*H,scale2*W)\n",
    "    \n",
    "    for i in range(N):\n",
    "        output[i] = torch.nn.functional.fold(input[i].view(C,1,-1).repeat(1,scale1*scale2,1), \n",
    "                                             output_size=(H*scale1,W*scale2), kernel_size = (scale1,scale2), \n",
    "                                             stride = (scale1,scale2) ).view(C,H*scale1,W*scale2)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e30fd5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NearestUpsampling(Module):\n",
    "        \n",
    "    def __init__(self, scale_factor: None ):\n",
    "        self.scale_factor = scale_factor\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        \n",
    "        if (len(input.size()) == 4) :\n",
    "            return nearest_upsampling(self.input, self.scale_factor)\n",
    "\n",
    "        \n",
    "    \n",
    "    def backward(self,*gradwrtouput):\n",
    "        return 3\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10f8fd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "input = torch.normal(mean = torch.zeros((100,3,280, 280)),std = 1)\n",
    "\n",
    "m = torch.nn.Upsample(scale_factor=2, mode='nearest')\n",
    "mm = m(input)\n",
    "\n",
    "m_ = NearestUpsampling(2)\n",
    "mm_ = m_.forward(input)\n",
    "\n",
    "print(torch.allclose(mm,mm_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a875b039",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 65)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m65\u001b[0m\n\u001b[0;31m    def conv2d(input: Tensor, weight: Tensor, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import fold, unfold\n",
    "from torch import Tensor, empty\n",
    "\n",
    "def conv2d_(input: Tensor, weight: Tensor, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor:\n",
    "    # input is 4d tensor\n",
    "    \n",
    "    if isinstance(stride, int):\n",
    "        stride = (stride, stride)\n",
    "    if isinstance(padding, int):\n",
    "        padding = (padding, padding)\n",
    "    if isinstance(dilation, int):\n",
    "        dilation = (dilation, dilation)\n",
    "    \n",
    "    N = input.size(0)\n",
    "    H_in = input.size(-2)\n",
    "    W_in = input.size(-1)\n",
    "    \n",
    "    kernel_size = (weight.size(-2), weight.size(-1))\n",
    "    C_out = weight.size(0)\n",
    "    H_out = int((H_in + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1)\n",
    "    W_out = int((W_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1)\n",
    "        \n",
    "    unfolded = unfold(input, kernel_size=kernel_size, dilation=dilation, stride=stride, padding=padding)\n",
    "    \n",
    "    #wxb  = empty(N, C_out, unfolded.size(2))\n",
    "    #for ind, unfdd in enumerate(unfolded):\n",
    "    #    wxb[ind] =  weight.view(C_out, -1) @ unfdd + bias.view(-1,1)\n",
    "    \n",
    "    #wxb = einsum('nij,njk->nik', weight.view(1, C_out, -1).repeat(N, 1, 1), unfolded) + bias.view(1, -1, 1).repeat(N,1,1)\n",
    "    \n",
    "    wxb = weight.view(1, C_out, -1).repeat(N, 1, 1).matmul(unfolded) + bias.view(1, -1, 1).repeat(N,1,1)\n",
    "    \n",
    "    return wxb.view(N, C_out, H_out, W_out)\n",
    "\n",
    "#FALSE METHOD\n",
    "def grad_conv2d_weight(self,gradwrtouput):\n",
    "\n",
    "        N = self.input.shape[0]\n",
    "\n",
    "        grad_ = gradwrtouput.contiguous().repeat(1, self.in_channel // self.groups, 1,\n",
    "                                                      1)\n",
    "        grad_ = grad_.contiguous().view(\n",
    "            grad_.shape[0] * grad_.shape[1], 1, grad_.shape[2],\n",
    "            grad_.shape[3])\n",
    "\n",
    "        input = self.input.contiguous().view(1, self.input.shape[0] * self.input.shape[1],\n",
    "                                        self.input.shape[2], self.input.shape[3])\n",
    "\n",
    "        grad_weight = self.conv2d(input, grad_, None, self.dilation, self.padding,\n",
    "                                   self.stride, self.in_channel * N)\n",
    "\n",
    "        grad_weight = grad_weight.contiguous().view(\n",
    "            min_batch, grad_weight.shape[1] // min_batch, grad_weight.shape[2],\n",
    "            grad_weight.shape[3])\n",
    "\n",
    "        return grad_weight.sum(dim=0).view(\n",
    "            self.in_channel // self.groups, self.out_channel,\n",
    "            grad_weight.shape[2], grad_weight.shape[3]).transpose(0, 1).narrow(\n",
    "                2, 0, self.kernel[2]).narrow(3, 0, self.kernel[3])\n",
    "    \n",
    "\n",
    "    \n",
    "        @staticmethod\n",
    "    def conv2d(input: Tensor, weight: Tensor, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor:\n",
    "        # input is 4d tensor\n",
    "\n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride, stride)\n",
    "        if isinstance(padding, int):\n",
    "            padding = (padding, padding)\n",
    "        if isinstance(dilation, int):\n",
    "            dilation = (dilation, dilation)\n",
    "\n",
    "        N = input.size(0)\n",
    "        H_in = input.size(-2)\n",
    "        W_in = input.size(-1)\n",
    "\n",
    "        kernel_size = (weight.size(-2), weight.size(-1))\n",
    "        C_out = weight.size(0)\n",
    "        H_out = int((H_in + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1)\n",
    "        W_out = int((W_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1)\n",
    "\n",
    "        unfolded = unfold(input, kernel_size=kernel_size, dilation=dilation, stride=stride, padding=padding)\n",
    "        \n",
    "        print(unfolded.size(),weight.view(1, C_out, -1).repeat(N, 1, 1).size())\n",
    "        \n",
    "        if bias != None:\n",
    "            wxb = weight.view(1, C_out, -1).repeat(N, 1, 1).matmul(unfolded.repeat_interleave(groups,1)) #+ ...\n",
    "            #bias.view(1, -1, 1).repeat(N,1,1)\n",
    "        else: \n",
    "            wxb = weight.view(1, C_out, -1).repeat(N, 1, 1).matmul(unfolded)\n",
    "        #print(wxb.size())\n",
    "        return wxb.view(N, C_out, H_out, W_out)\n",
    "\n",
    "\n",
    "\n",
    "    def conv2d(input: Tensor, weight: Tensor, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor:\n",
    "        # input is 4d tensor\n",
    "        \n",
    "\n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride, stride)\n",
    "        if isinstance(padding, int):\n",
    "            padding = (padding, padding)\n",
    "        if isinstance(dilation, int):\n",
    "            dilation = (dilation, dilation)\n",
    "\n",
    "        N = input.size(0)\n",
    "        C_in = input.size(1)\n",
    "        H_in = input.size(-2)\n",
    "        W_in = input.size(-1)\n",
    "        \n",
    "        \n",
    "        kernel_size = (weight.size(-2), weight.size(-1))\n",
    "        C_out = weight.size(0)\n",
    "        H_out = int((H_in + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1)\n",
    "        W_out = int((W_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1)\n",
    "        \n",
    "        #weight = weight.repeat_interleave(groups,1)\n",
    "        \n",
    "        inp_unf = unfold(input,kernel,stride,padding,dilation)\n",
    "        print(\"weight\", weight.view(C_out, -1).t().size())\n",
    "        print(\"inp_\", inp_unf.transpose(1, 2).size())\n",
    "        if bias != None:\n",
    "            out_unf = inp_unf.transpose(1, 2).matmul(weight.view(C_out, -1).t()).transpose(1, 2).add(bias.view(1, -1, 1).repeat(N,1,H_out*W_out))\n",
    "        else:\n",
    "            out_unf = inp_unf.transpose(1, 2).matmul(weight.view(C_out, -1).t()).transpose(1, 2)\n",
    "            \n",
    "        out = fold(out_unf, (H_out,W_out), (1,1), dilation, padding, stride)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def conv_backward(x, w, b, conv_param, dout):\n",
    "        HF, WF, DF, NF = w.shape\n",
    "        x_col = im2col(x, HF, WF, conv_param['pad'], conv_param['stride'])\n",
    "        w_col = w.transpose(3, 0, 1, 2).reshape((NF, -1))\n",
    "        db = np.sum(dout, axis=(0, 1, 3))\n",
    "        dout = dout.transpose(2, 0, 1, 3)\n",
    "        dout = dout.reshape((w_col.shape[0], x_col.shape[-1]))\n",
    "        dx_col = w_col.T.dot(dout)\n",
    "        dw_col = dout.dot(x_col.T)\n",
    "\n",
    "        dx = col2im(dx_col, x.shape, HF, WF, conv_param['pad'], conv_param['stride'])\n",
    "        dw = dw_col.reshape((dw_col.shape[0], HF, WF, DF))\n",
    "        dw = dw.transpose(1, 2, 3, 0)\n",
    "\n",
    "        return [dx, dw, db]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864e98c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def grad_conv2d_weight(self,input, weight_size, grad_output, stride=1, padding=0, dilation=1, groups=1):\n",
    "\n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride, stride)\n",
    "        if isinstance(padding, int):\n",
    "            padding = (padding, padding)\n",
    "        if isinstance(dilation, int):\n",
    "            dilation = (dilation, dilation)\n",
    "\n",
    "        N = input.size(0)\n",
    "        C_in = input.size(1)\n",
    "        H_in = input.size(-2)\n",
    "        W_in = input.size(-1)\n",
    "        \n",
    "        kernel_size = (weight_size[2], weight_size[3])\n",
    "        C_out = weight_size[0]\n",
    "        H_out = int((H_in + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1)\n",
    "        W_out = int((W_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1)\n",
    "\n",
    "        grad_w = torch.empty(weight_size)\n",
    "\n",
    "        for i in range(grad_w.size(0)): \n",
    "            for j in range(input.size(1)): \n",
    "                grad_w[i,j,:,:] = self.conv2d(self.input[:,j,:,:].view(N,1,H_in,W_in),\n",
    "                                              weight = grad_output[i,:,:].view(1,1,H_out,W_out), \n",
    "                                              bias = None,stride =self.dilation,padding = self.padding, \n",
    "                                              dilation = self.stride,groups = self.groups).narrow(2, 0, \n",
    "                                              weight_size[-2]).narrow(3, 0, weight_size[-1]).sum(dim=0)\n",
    "        \n",
    "        return grad_w\n",
    "    \n",
    "    def conv2d(input: Tensor, weight: Tensor, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor:\n",
    "        # input is 4d tensor\n",
    "        \n",
    "\n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride, stride)\n",
    "        if isinstance(padding, int):\n",
    "            padding = (padding, padding)\n",
    "        if isinstance(dilation, int):\n",
    "            dilation = (dilation, dilation)\n",
    "\n",
    "        N = input.size(0)\n",
    "        C_in = input.size(1)\n",
    "        H_in = input.size(-2)\n",
    "        W_in = input.size(-1)\n",
    "        \n",
    "        \n",
    "        kernel_size = (weight.size(-2), weight.size(-1))\n",
    "        C_out = weight.size(0)\n",
    "        H_out = int((H_in + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1)\n",
    "        W_out = int((W_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1)\n",
    "        \n",
    "        inp_unf = unfold(input,kernel_size,dilation,padding,stride)\n",
    "        if bias != None:\n",
    "            out_unf = inp_unf.transpose(1, 2).matmul(weight.view(C_out, -1).t()).transpose(1, 2).add(bias.view(1, -1, 1).repeat(N,1,H_out*W_out))\n",
    "        else:\n",
    "            out_unf = inp_unf.transpose(1, 2).matmul(weight.view(C_out, -1).t()).transpose(1, 2)\n",
    "        return out_unf.view(N,C_out,H_out,W_out)\n",
    "    \n",
    "    def _conv_transpose2d(input: Tensor, weight: Tensor, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1, output_padding=0) -> Tensor:\n",
    "    # input is 4d tensor\n",
    "    if isinstance(stride, int):\n",
    "        stride = (stride, stride)    \n",
    "    if isinstance(padding, int):\n",
    "        padding = (padding, padding)\n",
    "    if isinstance(dilation, int):\n",
    "        dilation = (dilation, dilation)\n",
    "    if (output_padding>=stride[0] or output_padding>=stride[1]) & (output_padding>=dilation[0] or output_padding>=dilation[1]):\n",
    "        raise ValueError(\"Invalid output_padding,output padding must be smaller than either stride or dilation\")\n",
    "    \n",
    "    N = input.size(0)\n",
    "    H_in = input.size(-2)\n",
    "    W_in = input.size(-1)\n",
    "    \n",
    "    kernel_size = (weight.size(-2), weight.size(-1))\n",
    "    C_out = weight.size(1)\n",
    "    H_out = (H_in - 1) * stride[0] - 2 * padding[0] + dilation[0] * (kernel_size[0] - 1) + 1\n",
    "    W_out = (W_in - 1) * stride[1] - 2 * padding[1] + dilation[1] * (kernel_size[1] - 1) + 1\n",
    "    \n",
    "    H_out_ = H_out + output_padding\n",
    "    W_out_ = W_out + output_padding\n",
    "    #print(H_out_)\n",
    "    pad0 = (dilation[0] * (kernel_size[0] - 1) - padding[0]) \n",
    "    pad1 = (dilation[1] * (kernel_size[1] - 1) - padding[1])\n",
    "\n",
    "    if (pad0<0) or (pad1<0):\n",
    "        raise ValueError(\"Invalid inputs, transposed convolution not possible\")\n",
    "    \n",
    "    if (stride[0] != 1) or (stride[1]!=1):\n",
    "        inputs = torch.empty(N,input.size(1),stride[0]*H_in-1,stride[1]*W_in-1).fill_(0.)\n",
    "        for i in range(H_in):\n",
    "            for j in range(W_in):\n",
    "                inputs[...,i*stride[0],j*stride[1]] = input[...,i,j]\n",
    "    else:\n",
    "        inputs = input\n",
    "    \n",
    "    unfolded = unfold(inputs, kernel_size=kernel_size, dilation=dilation,stride = 1, padding=(pad0,pad1))\n",
    "    \n",
    "    w = weight.transpose(0,1).rot90(2, [-2,-1])\n",
    "    \n",
    "    wxb = unfolded.transpose(1, 2).matmul(w.reshape(w.size(0), -1).t()).transpose(1, 2)\n",
    "    print(wxb.size(-1))\n",
    "    if (wxb.size(-1) == H_out_*W_out_):\n",
    "        print(1)\n",
    "        return wxb.view(N,C_out, H_out_, W_out_) + bias.view(1, -1, 1,1).repeat(N,1,1,1)\n",
    "    else:\n",
    "        find_size = True\n",
    "        H_out_effective = H_out_ - 1\n",
    "        W_out_effective = W_out_ - 1\n",
    "        \n",
    "        while (find_size):\n",
    "            if(wxb.size(-1) == H_out_effective*W_out_effective):\n",
    "                find_size = False\n",
    "        wxb = wxb.view(N,C_out,H_out_effective,W_out_effective)\n",
    "        print(\"sizeee: \", wxb.size())\n",
    "        if (output_padding!=0):\n",
    "            wxb_ = torch.empty(N,C_out,H_out_,W_out_).fill_(0.)\n",
    "            wxb_[...,0:wxb.size(-2),0:wxb.size(-1)] = wxb\n",
    "            return wxb_ + bias.view(1, -1, 1,1).repeat(N,1,1,1)\n",
    "        else:\n",
    "            return wxb[...,0:H_out,0:W_out] + bias.view(1, -1, 1,1).repeat(N,1,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "id": "250c0877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import fold, unfold\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class Conv2d(Module):\n",
    "    \n",
    "    def __init__(self,in_channel, out_channel, kernel_size = (2,2),stride=1, padding=0, dilation=1, groups=1,weight = None, bias = None):\n",
    "        if isinstance(stride, int):\n",
    "            self.stride = (stride, stride)\n",
    "        else:\n",
    "            self.stride = stride\n",
    "        if isinstance(padding, int):\n",
    "            self.padding = (padding, padding)\n",
    "        else: \n",
    "            self.padding = padding\n",
    "        if isinstance(dilation, int):\n",
    "            self.dilation = (dilation, dilation)\n",
    "        else:\n",
    "            self.dilation = dilation\n",
    "        if isinstance(kernel_size, int):\n",
    "            self.kernel = (kernel_size, kernel_size)\n",
    "        else:\n",
    "            self.kernel = kernel_size\n",
    "        \n",
    "        self.groups = groups\n",
    "        self.in_channel = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        \n",
    "        k = self.groups/(self.in_channel*self.kernel.prod())\n",
    "        if weight == None:\n",
    "            self.weight = torch.empty((self.out_channel,self.in_channel// self.groups,\n",
    "                                       self.kernel[0],self.kernel[1])).uniform_(-k,k)\n",
    "        else:\n",
    "            self.weight = weight\n",
    "        if bias == None:\n",
    "            self.bias = torch.empty(self.out_channel).uniform_(-k,k)\n",
    "        else:\n",
    "            self.bias = bias\n",
    "        \n",
    "        self.weight_grad = torch.empty((self.kernel[0],self.kernel[1]))\n",
    "        self.bias_grad = torch.empty(self.out_channel)\n",
    "        \n",
    "    @staticmethod\n",
    "    def conv2d(input: Tensor, weight: Tensor, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor:\n",
    "        # input is 4d tensor\n",
    "        \n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride, stride)\n",
    "        if isinstance(padding, int):\n",
    "            padding = (padding, padding)\n",
    "        if isinstance(dilation, int):\n",
    "            dilation = (dilation, dilation)\n",
    "\n",
    "        N = input.size(0)\n",
    "        C_in = input.size(1)\n",
    "        H_in = input.size(-2)\n",
    "        W_in = input.size(-1)\n",
    "        \n",
    "        kernel_size = (weight.size(-2), weight.size(-1))\n",
    "        C_out = weight.size(0)\n",
    "        H_out = int((H_in + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1)\n",
    "        W_out = int((W_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1)\n",
    "       \n",
    "        out_conv = torch.empty(N,C_out,H_out,W_out)\n",
    "        \n",
    "        inp_unf = unfold(input.view(N*groups,C_in//groups,H_in,W_in),kernel_size,dilation,padding,stride)#.view(N,H_out*W_out,-1,C_in//groups)\n",
    "        inp_unf = inp_unf.view(N,groups,inp_unf.size(-2),inp_unf.size(-1)).transpose(0,1)\n",
    "        \n",
    "        \n",
    "        weight = weight.view(groups,C_out//groups,C_in//groups,kernel_size[0],kernel_size[1])\n",
    "        \n",
    "        for i in range(groups):\n",
    "            out_unf = inp_unf[i,...].transpose(1,2).matmul(weight[i,...].view(C_out//groups,-1).t()).transpose(1,2)\n",
    "            out_conv[:,i*(C_out//groups):(1+i)*(C_out//groups),...] = out_unf.view(N,C_out//groups,H_out,W_out)\n",
    "        if bias!=None:\n",
    "            return out_conv + bias.view(1, -1, 1,1).repeat(N,1,H_out,W_out)\n",
    "        return out_conv\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def conv_transpose2d(input: Tensor, weight: Tensor, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1, output_padding=0) -> Tensor:\n",
    "        # input is 4d tensor\n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride, stride)    \n",
    "        if isinstance(padding, int):\n",
    "            padding = (padding, padding)\n",
    "        if isinstance(output_padding, int):\n",
    "            output_padding = (output_padding, output_padding)\n",
    "        if isinstance(dilation, int):\n",
    "            dilation = (dilation, dilation)\n",
    "\n",
    "        N = input.size(0)\n",
    "        H_in = input.size(-2)\n",
    "        W_in = input.size(-1)\n",
    "\n",
    "        kernel_size = (weight.size(-2), weight.size(-1))\n",
    "        C_out = weight.size(1)\n",
    "        H_out = (H_in - 1) * stride[0] - 2 * padding[0] + dilation[0] * (kernel_size[0] - 1) + output_padding[0] + 1\n",
    "        W_out = (W_in - 1) * stride[1] - 2 * padding[1] + dilation[1] * (kernel_size[1] - 1) + output_padding[1] + 1\n",
    "\n",
    "        pad0 = (dilation[0] * (kernel_size[0] - 1) - padding[0]) * stride[0]\n",
    "        pad1 = (dilation[1] * (kernel_size[1] - 1) - padding[1]) * stride[1]\n",
    "\n",
    "        if (pad0<0) or (pad1<0):\n",
    "            raise ValueError(\"Invalid inputs, transposed convolution not possible\")\n",
    "        inputs = torch.empty(N,input.size(1),2*H_in-1,2*W_in-1)\n",
    "        for i in range(7):\n",
    "            inputs[:,:,]\n",
    "\n",
    "        unfolded = unfold(input, kernel_size=kernel_size, dilation=dilation,stride = stride, padding=(pad0,pad1))\n",
    "\n",
    "        w = weight.transpose(0,1).rot90(2, [-2,-1])\n",
    "        if bias != None:\n",
    "            wxb = unfolded.transpose(1, 2).matmul(w.reshape(w.size(0), -1).t()).transpose(1, 2) + bias.view(1, -1, 1).repeat(N,1,1)\n",
    "        else: \n",
    "            wxb = unfolded.transpose(1, 2).matmul(w.reshape(w.size(0), -1).t()).transpose(1, 2)\n",
    "        return fold(wxb,(H_out, W_out), (1,1),stride = stride,dilation = dilation)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        \n",
    "        if (len(input.size()) == 4) :\n",
    "            return self.conv2d(self.input,weight = self.weight, bias = self.bias,stride =self.stride,\n",
    "                                  padding = self.padding, dilation = self.dilation,groups = self.groups)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def grad_con2d_input(self,input, weight_size, grad_output, stride=1, padding=0, dilation=1, groups=1):\n",
    "\n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride, stride)\n",
    "        if isinstance(padding, int):\n",
    "            padding = (padding, padding)\n",
    "        if isinstance(dilation, int):\n",
    "            dilation = (dilation, dilation)\n",
    "\n",
    "        N = input.size(0)\n",
    "        C_in = input.size(1)\n",
    "        H_in = input.size(-2)\n",
    "        W_in = input.size(-1)\n",
    "        \n",
    "        kernel_size = (weight_size[2], weight_size[3])\n",
    "        C_out = weight_size[0]\n",
    "        H_out = int((H_in + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1)\n",
    "        W_out = int((W_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1)\n",
    "        \n",
    "        grad_input = torch.empty(weight_size)\n",
    "\n",
    "        for i in range(grad_input.size(0)): \n",
    "            for j in range(input.size(1)): \n",
    "                grad_input[i,j,:,:] = self.conv_transpose2d(self.input[:,j,:,:].view(N,1,H_in,W_in),\n",
    "                                              weight = grad_output[:,i,:,:].view(N,1,H_out,W_out), \n",
    "                                              bias = None,stride =self.stride,padding = self.padding, \n",
    "                                              dilation = self.dilation).narrow(2, 0, \n",
    "                                              H_in).narrow(3, 0, W_in)\n",
    "        \n",
    "        return grad_input\n",
    "    \n",
    "    def grad_conv2d_weight(self,input, weight_size, grad_output, stride=1, padding=0, dilation=1, groups=1):\n",
    "        r\"\"\"\n",
    "        Computes the gradient of conv2d with respect to the weight of the convolution.\n",
    "        Args:\n",
    "            input: input tensor of shape (minibatch x in_channels x iH x iW)\n",
    "            weight_size : Shape of the weight gradient tensor\n",
    "            grad_output : output gradient tensor (minibatch x out_channels x oH x oW)\n",
    "            stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
    "            padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0\n",
    "            dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
    "            groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n",
    "        \"\"\"\n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride, stride)\n",
    "        if isinstance(padding, int):\n",
    "            padding = (padding, padding)\n",
    "        if isinstance(dilation, int):\n",
    "            dilation = (dilation, dilation)\n",
    "        in_channels = input.shape[1]\n",
    "        out_channels = grad_output.shape[1]\n",
    "        N = input.shape[0]\n",
    "\n",
    "        grad_output = grad_output.repeat(1, in_channels // groups, 1,\n",
    "                                                      1)\n",
    "        grad_output = grad_output.view(\n",
    "            grad_output.shape[0] * grad_output.shape[1], 1, grad_output.shape[2],\n",
    "            grad_output.shape[3])\n",
    "\n",
    "        input = input.view(1, input.shape[0] * input.shape[1],\n",
    "                                        input.shape[2], input.shape[3])\n",
    "\n",
    "        grad_weight = conv2d(input, grad_output, None, dilation, padding,\n",
    "                                   stride, in_channels * N)\n",
    "\n",
    "        grad_weight = grad_weight.view(\n",
    "            N, grad_weight.shape[1] // N, grad_weight.shape[2],\n",
    "            grad_weight.shape[3])\n",
    "\n",
    "        return grad_weight.sum(dim=0).view(\n",
    "            in_channels // groups, out_channels,\n",
    "            grad_weight.shape[2], grad_weight.shape[3]).transpose(0, 1).narrow(\n",
    "                2, 0, weight_size[2]).narrow(3, 0, weight_size[3])\n",
    "\n",
    "    def grad_conv_bias(self,grad_output,bias_size):\n",
    "        \n",
    "        return grad_output.transpose(0,1).reshape(grad_output.size(1),-1).sum(dim = 1)\n",
    "    \n",
    "    def backward(self,gradwrtouput):\n",
    "        bias = self.grad_conv_bias(gradwrtouput,self.bias.shape)\n",
    "        a = self.grad_conv2d_weight(self.input, self.weight.shape, gradwrtouput, stride = self.stride, padding=self.padding, dilation=self.dilation, groups=self.groups)\n",
    "        #b = self.conv_transpose2d(self.input, self.weight, gradwrtouput,stride = self.stride,padding = self.padding,dilation = self.dilation)\n",
    "        return bias\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "id": "57d4fa66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([110, 2, 142, 142])\n",
      "torch.Size([110, 2, 142, 142])\n",
      "tensor(0.0001)\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    input = torch.normal(mean = torch.zeros(110,4,280,280),std = 1)\n",
    "\n",
    "    kernel = torch.tensor([2,2])\n",
    "    stride = 2\n",
    "    padding = 2\n",
    "    dilation= 1\n",
    "    groups = 2\n",
    "\n",
    "    conv = torch.nn.Conv2d(4, 2, kernel, stride=stride, padding=padding, dilation=dilation, groups=groups)\n",
    "    weight = conv.state_dict()[\"weight\"]\n",
    "    bias = conv.state_dict()[\"bias\"]\n",
    "    #print(bias.size())\n",
    "    conv_ = Conv2d(4,2,kernel,stride, padding, dilation, groups,weight, bias)\n",
    "\n",
    "    m = conv.forward(input) \n",
    "    #print(\"size\",m.size())\n",
    "    m_ = conv_.forward(input)\n",
    "    #m = torch.conv2d(input, weight, None, stride, padding,dilation, groups)\n",
    "    #print(torch.allclose(m,m_))\n",
    "    print(torch.norm(m-m_))\n",
    "\n",
    "\n",
    "grad_output = torch.normal(mean=torch.zeros(m.size()),std= 1)\n",
    "with torch.no_grad():\n",
    "    grad = conv_.backward(grad_output)\n",
    "print(grad.size())\n",
    "\n",
    "#input.requires_grad_(True)\n",
    "#weight.requires_grad_(True)\n",
    "    \n",
    "#output = torch.nn.functional.conv2d(input, weight)\n",
    "#grad_weight = torch.autograd.grad(output, filter, grad_output)\n",
    "#print(input.size(), weight.shape, grad_output.size())\n",
    "#torch.nn.functional.grad.conv2d_weight(input, weight.shape, grad_output,stride = stride,dilation=dilation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "id": "a33ab0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 966,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "id": "756e1907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def _conv_transpose2d(input: Tensor, weight: Tensor, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1, output_padding=0) -> Tensor:\n",
    "    # input is 4d tensor\n",
    "    if isinstance(stride, int):\n",
    "        stride = (stride, stride)    \n",
    "    if isinstance(padding, int):\n",
    "        padding = (padding, padding)\n",
    "    if isinstance(dilation, int):\n",
    "        dilation = (dilation, dilation)\n",
    "    if (output_padding>=stride[0] or output_padding>=stride[1]) & (output_padding>=dilation[0] or output_padding>=dilation[1]):\n",
    "        raise ValueError(\"Invalid output_padding,output padding must be smaller than either stride or dilation\")\n",
    "    \n",
    "    N = input.size(0)\n",
    "    H_in = input.size(-2)\n",
    "    W_in = input.size(-1)\n",
    "    \n",
    "    kernel_size = (weight.size(-2), weight.size(-1))\n",
    "    C_out = weight.size(1)\n",
    "    H_out = (H_in - 1) * stride[0] - 2 * padding[0] + dilation[0] * (kernel_size[0] - 1) + 1\n",
    "    W_out = (W_in - 1) * stride[1] - 2 * padding[1] + dilation[1] * (kernel_size[1] - 1) + 1\n",
    "    \n",
    "    H_out_ = H_out + output_padding\n",
    "    W_out_ = W_out + output_padding\n",
    "    #print(H_out_)\n",
    "    pad0 = (dilation[0] * (kernel_size[0] - 1) - padding[0]) \n",
    "    pad1 = (dilation[1] * (kernel_size[1] - 1) - padding[1])\n",
    "\n",
    "    if (pad0<0) or (pad1<0):\n",
    "        raise ValueError(\"Invalid inputs, transposed convolution not possible\")\n",
    "    \n",
    "    if (stride[0] != 1) or (stride[1]!=1):\n",
    "        inputs = torch.empty(N,input.size(1),stride[0]*H_in-1,stride[1]*W_in-1).fill_(0.)\n",
    "        for i in range(H_in):\n",
    "            for j in range(W_in):\n",
    "                inputs[...,i*stride[0],j*stride[1]] = input[...,i,j]\n",
    "    else:\n",
    "        inputs = input\n",
    "    \n",
    "    unfolded = unfold(inputs, kernel_size=kernel_size, dilation=dilation,stride = 1, padding=(pad0,pad1))\n",
    "    \n",
    "    w = weight.transpose(0,1).rot90(2, [-2,-1])\n",
    "    \n",
    "    wxb = unfolded.transpose(1, 2).matmul(w.reshape(w.size(0), -1).t()).transpose(1, 2)\n",
    "    print(wxb.size(-1))\n",
    "    if (wxb.size(-1) == H_out_*W_out_):\n",
    "        return wxb.view(N,C_out, H_out_, W_out_) + bias.view(1, -1, 1,1).repeat(N,1,1,1)\n",
    "    else:\n",
    "        find_size = True\n",
    "        H_out_effective = H_out_ - 1\n",
    "        W_out_effective = W_out_ - 1\n",
    "        \n",
    "        while (find_size):\n",
    "            if(wxb.size(-1) == H_out_effective*W_out_effective):\n",
    "                find_size = False\n",
    "        wxb = wxb.view(N,C_out,H_out_effective,W_out_effective)\n",
    "        print(\"sizeee: \", wxb.size())\n",
    "        if (output_padding!=0):\n",
    "            wxb_ = torch.empty(N,C_out,H_out_,W_out_).fill_(0.)\n",
    "            wxb_[...,0:wxb.size(-2),0:wxb.size(-1)] = wxb\n",
    "            return wxb_ + bias.view(1, -1, 1,1).repeat(N,1,1,1)\n",
    "        else:\n",
    "            return wxb[...,0:H_out,0:W_out] + bias.view(1, -1, 1,1).repeat(N,1,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "id": "b322868f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169\n",
      "sizeee:  torch.Size([10, 2, 13, 13])\n",
      "tensor([[ 0.0984,  0.0478, -0.0699, -0.3200,  0.2897,  0.0773,  0.2755,  0.3862,\n",
      "         -0.0544, -0.0865, -0.0211,  0.0853,  0.0550,  0.0468],\n",
      "        [ 0.2963,  0.4745,  0.1291, -0.4850, -0.2147, -0.3885,  0.4238,  0.6988,\n",
      "          0.1219, -0.2336, -0.1225, -0.0623, -0.2228,  0.0468],\n",
      "        [-0.1624,  0.0535,  0.2761,  0.1379,  0.1224, -0.1374, -0.0238,  0.0137,\n",
      "          0.3729,  0.0319, -0.2822, -0.1254, -0.0122,  0.0468],\n",
      "        [-0.0507,  0.5516, -0.3158,  0.2655,  0.7902,  0.0331,  0.5681,  0.7517,\n",
      "         -0.2997,  0.4871,  0.2907, -0.0139,  0.0170,  0.0468],\n",
      "        [-0.0163,  0.0363,  0.0076,  0.5989,  0.2335, -0.2862,  0.1563,  0.0633,\n",
      "          0.0099,  0.8350, -0.1404,  0.1876,  0.0928,  0.0468],\n",
      "        [ 0.1899,  0.0122,  0.0342,  1.0551,  0.3656, -0.1435, -0.0442,  0.5517,\n",
      "         -0.0759,  0.4776, -0.5041, -0.0044, -0.4174,  0.0468],\n",
      "        [ 0.1217,  0.0268, -0.0597,  0.1811,  0.1272,  0.3913,  0.0080,  0.3386,\n",
      "          0.0773,  0.1081,  0.4785,  0.4663, -0.0954,  0.0468],\n",
      "        [-0.4511,  0.3690,  0.0853, -0.4069,  0.2448, -0.5052,  0.2176, -0.2092,\n",
      "         -0.1820, -0.6812, -0.9663, -0.1136,  0.2832,  0.0468],\n",
      "        [ 0.1131,  0.2717, -0.0459,  0.1239, -0.1848,  0.2261,  0.1909,  0.0865,\n",
      "         -0.3473,  0.3220, -0.1772,  0.1551,  0.1829,  0.0468],\n",
      "        [-0.1231, -0.4789,  0.1582, -0.2601, -0.0065,  0.4930,  0.0548, -0.3286,\n",
      "         -0.7492, -0.1821, -0.3323, -0.1374,  0.1766,  0.0468],\n",
      "        [ 0.1173, -0.2576,  0.2094, -0.4010, -0.0451,  0.0636, -0.2006,  0.2734,\n",
      "         -0.2376,  0.3395, -0.4166, -0.2898, -0.0267,  0.0468],\n",
      "        [ 0.0661, -0.2316,  0.5165,  0.1071,  0.1916, -0.2745, -0.0204,  0.2370,\n",
      "         -0.1855,  0.1631,  0.1644,  0.2604, -0.1509,  0.0468],\n",
      "        [ 0.0725, -0.0469, -0.0258, -0.2617,  0.1056,  0.0990,  0.0937,  0.2524,\n",
      "          0.0504, -0.0675,  0.1611, -0.1880,  0.0493,  0.0468],\n",
      "        [ 0.0468,  0.0468,  0.0468,  0.0468,  0.0468,  0.0468,  0.0468,  0.0468,\n",
      "          0.0468,  0.0468,  0.0468,  0.0468,  0.0468,  0.0468]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(5.6182, grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn(10, 4,6,6)\n",
    "\n",
    "    \n",
    "stride = 2\n",
    "padding = 1\n",
    "dilation= 1\n",
    "output_padding = 1\n",
    "groups = 1\n",
    "\n",
    "tt = torch.nn.ConvTranspose2d(4,2,(5,5),stride= stride,dilation =dilation,padding = padding\n",
    "                              ,output_padding = output_padding)\n",
    "weights = tt.weight\n",
    "bias = tt.bias\n",
    "\n",
    "trans_conv = tt(inputs)\n",
    "#print(trans_conv[1,1,...])\n",
    "trans_conv_ = _conv_transpose2d(inputs,weights,stride= stride,dilation =dilation,padding = padding,\n",
    "                                bias = bias,output_padding=output_padding)\n",
    "print(trans_conv_[1,1,...])\n",
    "print((trans_conv-trans_conv_).norm())\n",
    "\n",
    "#print(torch.nn.functional.conv_transpose2d(inputs, weights,output_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "id": "0ddda658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 931,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13*13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "id": "7ca8ee15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "sizeee:  torch.Size([1, 1, 5, 5])\n",
      "tensor(0.2083, grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([1.,2.,3.,4.,1.,2.,3.,4.]).view(1,2,2,2)\n",
    "stride = 2\n",
    "padding = 1\n",
    "dilation= 1\n",
    "output_padding = 1\n",
    "#weight = torch.arange(0.,18).view(1,2,3,3)\n",
    "conv = torch.nn.ConvTranspose2d(2,1,(3,3),stride= stride,dilation = dilation,output_padding=output_padding,bias = False)\n",
    "\n",
    "state_dict_ = conv.state_dict()\n",
    "\n",
    "weight = state_dict_['weight'] \n",
    "#state_dict_[\"bias\"] = torch.zeros(1)\n",
    "conv.load_state_dict(state_dict_)\n",
    "ccc =conv(inputs)\n",
    "\n",
    "trans_conv_ = _conv_transpose2d(inputs,weight,stride= stride,dilation = dilation,bias = bias,output_padding=output_padding)\n",
    "#print(trans_conv_)\n",
    "#print(cc.size(),trans_conv_.size())\n",
    "print((ccc-trans_conv_).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "id": "002b45a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  0.,  0.,  1.,  0.,  2.,  2.,  0.,  4.],\n",
       "          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  3.,  0.,  4.,  6.,  0.,  8.],\n",
       "          [ 3.,  0.,  6.,  4.,  0.,  8.,  5.,  0., 10.],\n",
       "          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "          [ 9.,  0., 12., 12.,  0., 16., 15.,  0., 20.],\n",
       "          [ 6.,  0., 12.,  7.,  0., 14.,  8.,  0., 16.],\n",
       "          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "          [18.,  0., 24., 21.,  0., 28., 24.,  0., 32.]]]])"
      ]
     },
     "execution_count": 790,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_conv_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "id": "ad73c792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pair(x):\n",
    "    if isinstance(x, int):\n",
    "        return (x, x)\n",
    "    return x\n",
    "\n",
    "def pad(input, pad=(1,1,1,1), mode=\"constant\", value=0.0):\n",
    "    # pad = (pad_left, pad_right, pad_up, pad_down)\n",
    "    if len(pad) == 2:\n",
    "        pad = (pad[0], pad[0], pad[1], pad[1])\n",
    "    input_shape = [input.size(0), input.size(1), input.size(2), input.size(3)]\n",
    "    \n",
    "    input_shape[3] += (pad[0] + pad[1])\n",
    "    input_shape[2] += (pad[2] + pad[3])\n",
    "    i1 = pad[2]\n",
    "    j1 = pad[0]\n",
    "    i2 = input_shape[2] - pad[3]\n",
    "    j2 = input_shape[3] - pad[1]\n",
    "    \n",
    "    result = empty(input_shape).fill_(value)\n",
    "    result[:,:,i1:i2,j1:j2] = input\n",
    "    return result\n",
    "\n",
    "def zero_internal_pad(x, pad=(1,1)):\n",
    "    x_shape = [x.size(0), x.size(1), x.size(2), x.size(3)]\n",
    "    x_shape[2] += (pad[0]*(x_shape[2]-1)) \n",
    "    x_shape[3] += (pad[1]*(x_shape[3]-1)) \n",
    "    res = empty(x_shape).fill_(0.0)\n",
    "    res[:,:,::pad[0]+1,::pad[1]+1] = x\n",
    "    return res\n",
    "\n",
    "# TODO: groups not working\n",
    "def conv_transpose2d(input: Tensor, weight: Tensor, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1, output_padding=0) -> Tensor:\n",
    "    # input is 4d tensor\n",
    "    stride = _pair(stride)\n",
    "    padding = _pair(padding)\n",
    "    output_padding = _pair(output_padding)\n",
    "    dilation = _pair(dilation)\n",
    "\n",
    "    N = input.size(0)\n",
    "    H_in = input.size(-2)\n",
    "    W_in = input.size(-1)\n",
    "        \n",
    "    kernel_size = (weight.size(-2), weight.size(-1))\n",
    "    C_out = weight.size(1)\n",
    "    H_out = (H_in - 1) * stride[0] - 2 * padding[0] + dilation[0] * (kernel_size[0] - 1) + 1 + output_padding[0]\n",
    "    W_out = (W_in - 1) * stride[1] - 2 * padding[1] + dilation[1] * (kernel_size[1] - 1) + 1 + output_padding[1]\n",
    "    \n",
    "    pad0 = (dilation[0] * (kernel_size[0] - 1) - padding[0])\n",
    "    pad1 = (dilation[1] * (kernel_size[1] - 1) - padding[1])\n",
    "\n",
    "    if (pad0<0) or (pad1<0):\n",
    "        raise ValueError(\"Invalid inputs, transposed convolution not possible\")\n",
    "    \n",
    "    if (stride[0]>1) or (stride[1]>1):\n",
    "        input = zero_internal_pad(input, pad=(stride[0]-1,stride[1]-1))\n",
    "    if (output_padding[0]>0) or (output_padding[1]>0):\n",
    "        input = pad(input, pad=(0,output_padding[1],0,output_padding[0]))\n",
    "    unfolded = unfold(input, kernel_size=kernel_size, dilation=dilation, stride=1, padding=(pad0,pad1))\n",
    "    \n",
    "    w = weight.transpose(0,1).rot90(-2, [-2,-1])\n",
    "    \n",
    "    wxb = unfolded.transpose(1, 2).matmul(w.reshape(w.size(0), -1).t()).transpose(1, 2) + bias.view(1, -1, 1).repeat(N,1,1)\n",
    "    \n",
    "    res = wxb.view(N, C_out, H_out, W_out)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "id": "01dcfd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5515e-06, grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "from torch import empty\n",
    "inputs = torch.randn(10, 4,6,6)\n",
    "\n",
    "    \n",
    "stride = 1\n",
    "padding = 3\n",
    "dilation= 3\n",
    "output_padding = 2\n",
    "groups = 1\n",
    "\n",
    "tt = torch.nn.ConvTranspose2d(4,2,(5,5),stride= stride,dilation =dilation,padding = padding\n",
    "                              ,output_padding = output_padding)\n",
    "weights = tt.weight\n",
    "bias = tt.bias\n",
    "\n",
    "trans_conv = tt(inputs)\n",
    "#print(trans_conv[1,1,...])\n",
    "trans_conv_ = conv_transpose2d(inputs,weights,stride= stride,dilation =dilation,padding = padding,\n",
    "                                bias = bias,output_padding=output_padding)\n",
    "#print(trans_conv_[1,1,...])\n",
    "print((trans_conv-trans_conv_).norm())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
