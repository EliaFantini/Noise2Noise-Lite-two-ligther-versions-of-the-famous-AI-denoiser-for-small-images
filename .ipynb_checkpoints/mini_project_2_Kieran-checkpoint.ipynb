{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d5f64d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \n",
    "     def forward(self,*input):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "     def backward(self,*gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "     def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3868d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \n",
    "    def __init__(self, modules):\n",
    "        self.modules = modules\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        output = self.input\n",
    "        \n",
    "        for module in self.modules:\n",
    "            output = module.forward(output)\n",
    "        return output\n",
    "    \n",
    "    def backward(self,*gradwrtouput):\n",
    "        gradient = gradwrtouput\n",
    "        for module in reversed(self.modules):\n",
    "            gradient = module.backward(gradient)\n",
    "        self.input = None\n",
    "        return gradient\n",
    "    \n",
    "    def param(self):\n",
    "        params = []\n",
    "        for module in self.modules:\n",
    "            params.append(module.param())\n",
    "        return params\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a44146f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def step(self):\n",
    "        return NotImplementedError\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        return NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "94661872",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    \n",
    "    def __init__(self,params,lr, mu = 0, tau = 0):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        \n",
    "        # parameters in order to add momemtum \n",
    "        self.momemtum = mu\n",
    "        self.dampening = tau\n",
    "        self.state_momemtum = None\n",
    "        \n",
    "    def step(self):\n",
    "        for x, grad in self.params:\n",
    "            x.add_(-self.lr * grad) \n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for x, grad in self.params:\n",
    "            grad = grad.zero_()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "913048a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Module):\n",
    "    \n",
    "    def forward(self,input,target):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        return (self.input - self.target).pow(2).mean() \n",
    "    \n",
    "    def backward(self):\n",
    "        return 2*(self.input - self.target).div(torch.tensor(self.input.size(0)))\n",
    "        ## we divide by the batch size as in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "3c22c5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9524)\n",
      "tensor(1.9524)\n",
      "torch.Size([100, 3, 10])\n"
     ]
    }
   ],
   "source": [
    "#Check MSE loss function\n",
    "mse = MSE()\n",
    "input = torch.normal(mean=torch.zeros(100,3,10),std = 1)\n",
    "target = torch.normal(mean=torch.zeros(100,3,10),std = 1)\n",
    "loss = mse.forward(input,target)\n",
    "print(loss)\n",
    "mse_ = torch.nn.MSELoss()\n",
    "loss_ = mse_.forward(input,target)\n",
    "print(loss_)\n",
    "\n",
    "print(mse.backward().size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "668203f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    \n",
    "    def forward(self,input):\n",
    "        self.input = input\n",
    "        self.sigmoid = 1./(1+(-self.input).exp())\n",
    "        return  self.sigmoid\n",
    "    \n",
    "    def backward(self,*gradwrtouput):\n",
    "        return gradwrtouput*self.sigmoid*(1-self.sigmoid)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "088fd3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "input = torch.normal(mean=torch.zeros(100,3,38,38),std = 1)\n",
    "\n",
    "sig = Sigmoid()\n",
    "sig_ = torch.nn.Sigmoid()\n",
    "\n",
    "print(torch.allclose(sig.forward(input),sig_.forward(input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "2bb20769",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return (self.input>0.)*self.input\n",
    "    \n",
    "    def backward(self, *gradwrtouput):\n",
    "        return gradwrtouput*(self.input>=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "fd9e7c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "input = torch.normal(mean=torch.zeros(100,3,380,380),std = 1)\n",
    "\n",
    "re = ReLU()\n",
    "re_ = torch.nn.ReLU()\n",
    "\n",
    "print(torch.allclose(sig.forward(input),sig_.forward(input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a734070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c5d86f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_upsampling(input, scale_factor):\n",
    "    if isinstance(scale_factor,int):\n",
    "        scale1 ,scale2 = scale_factor,scale_factor\n",
    "    if isinstance(scale_factor,tuple):\n",
    "        scale1 ,scale2 = scale_factor[0],scale_factor[1]\n",
    "    \n",
    "    N, C, H, W = tuple(input.size())\n",
    "    output = torch.empty(N,C,scale1*H,scale2*W)\n",
    "    \n",
    "    for i in range(N):\n",
    "        output[i] = torch.nn.functional.fold(input[i].view(C,1,-1).repeat(1,scale1*scale2,1), \n",
    "                                             output_size=(H*scale1,W*scale2), kernel_size = (scale1,scale2), \n",
    "                                             stride = (scale1,scale2) ).view(C,H*scale1,W*scale2)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "e30fd5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NearestUpsampling(Module):\n",
    "        \n",
    "    def __init__(self, scale_factor: None ):\n",
    "        self.scale_factor = scale_factor\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        \n",
    "        if (len(input.size()) == 4) :\n",
    "            return nearest_upsampling(self.input, self.scale_factor)\n",
    "\n",
    "        \n",
    "    \n",
    "    def backward(self,*gradwrtouput):\n",
    "        return 3\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "10f8fd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "input = torch.normal(mean = torch.zeros((100,3,280, 280)),std = 1)\n",
    "\n",
    "m = torch.nn.Upsample(scale_factor=2, mode='nearest')\n",
    "mm = m(input)\n",
    "\n",
    "m_ = NearestUpsampling(2)\n",
    "mm_ = m_.forward(input)\n",
    "\n",
    "print(torch.allclose(mm,mm_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "id": "a875b039",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 65)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m65\u001b[0m\n\u001b[0;31m    def conv2d(input: Tensor, weight: Tensor, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import fold, unfold\n",
    "from torch import Tensor, empty, einsum\n",
    "\n",
    "\n",
    "def conv2d_(input: Tensor, weight: Tensor, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor:\n",
    "    # input is 4d tensor\n",
    "    \n",
    "    if isinstance(stride, int):\n",
    "        stride = (stride, stride)\n",
    "    if isinstance(padding, int):\n",
    "        padding = (padding, padding)\n",
    "    if isinstance(dilation, int):\n",
    "        dilation = (dilation, dilation)\n",
    "    \n",
    "    N = input.size(0)\n",
    "    H_in = input.size(-2)\n",
    "    W_in = input.size(-1)\n",
    "    \n",
    "    kernel_size = (weight.size(-2), weight.size(-1))\n",
    "    C_out = weight.size(0)\n",
    "    H_out = int((H_in + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1)\n",
    "    W_out = int((W_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1)\n",
    "        \n",
    "    unfolded = unfold(input, kernel_size=kernel_size, dilation=dilation, stride=stride, padding=padding)\n",
    "    \n",
    "    #wxb  = empty(N, C_out, unfolded.size(2))\n",
    "    #for ind, unfdd in enumerate(unfolded):\n",
    "    #    wxb[ind] =  weight.view(C_out, -1) @ unfdd + bias.view(-1,1)\n",
    "    \n",
    "    #wxb = einsum('nij,njk->nik', weight.view(1, C_out, -1).repeat(N, 1, 1), unfolded) + bias.view(1, -1, 1).repeat(N,1,1)\n",
    "    \n",
    "    wxb = weight.view(1, C_out, -1).repeat(N, 1, 1).matmul(unfolded) + bias.view(1, -1, 1).repeat(N,1,1)\n",
    "    \n",
    "    return wxb.view(N, C_out, H_out, W_out)\n",
    "\n",
    "#FALSE METHOD\n",
    "def grad_conv2d_weight(self,gradwrtouput):\n",
    "\n",
    "        N = self.input.shape[0]\n",
    "\n",
    "        grad_ = gradwrtouput.contiguous().repeat(1, self.in_channel // self.groups, 1,\n",
    "                                                      1)\n",
    "        grad_ = grad_.contiguous().view(\n",
    "            grad_.shape[0] * grad_.shape[1], 1, grad_.shape[2],\n",
    "            grad_.shape[3])\n",
    "\n",
    "        input = self.input.contiguous().view(1, self.input.shape[0] * self.input.shape[1],\n",
    "                                        self.input.shape[2], self.input.shape[3])\n",
    "\n",
    "        grad_weight = self.conv2d(input, grad_, None, self.dilation, self.padding,\n",
    "                                   self.stride, self.in_channel * N)\n",
    "\n",
    "        grad_weight = grad_weight.contiguous().view(\n",
    "            min_batch, grad_weight.shape[1] // min_batch, grad_weight.shape[2],\n",
    "            grad_weight.shape[3])\n",
    "\n",
    "        return grad_weight.sum(dim=0).view(\n",
    "            self.in_channel // self.groups, self.out_channel,\n",
    "            grad_weight.shape[2], grad_weight.shape[3]).transpose(0, 1).narrow(\n",
    "                2, 0, self.kernel[2]).narrow(3, 0, self.kernel[3])\n",
    "    \n",
    "\n",
    "    \n",
    "        @staticmethod\n",
    "    def conv2d(input: Tensor, weight: Tensor, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor:\n",
    "        # input is 4d tensor\n",
    "\n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride, stride)\n",
    "        if isinstance(padding, int):\n",
    "            padding = (padding, padding)\n",
    "        if isinstance(dilation, int):\n",
    "            dilation = (dilation, dilation)\n",
    "\n",
    "        N = input.size(0)\n",
    "        H_in = input.size(-2)\n",
    "        W_in = input.size(-1)\n",
    "\n",
    "        kernel_size = (weight.size(-2), weight.size(-1))\n",
    "        C_out = weight.size(0)\n",
    "        H_out = int((H_in + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1)\n",
    "        W_out = int((W_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1)\n",
    "\n",
    "        unfolded = unfold(input, kernel_size=kernel_size, dilation=dilation, stride=stride, padding=padding)\n",
    "        \n",
    "        print(unfolded.size(),weight.view(1, C_out, -1).repeat(N, 1, 1).size())\n",
    "        \n",
    "        if bias != None:\n",
    "            wxb = weight.view(1, C_out, -1).repeat(N, 1, 1).matmul(unfolded.repeat_interleave(groups,1)) #+ ...\n",
    "            #bias.view(1, -1, 1).repeat(N,1,1)\n",
    "        else: \n",
    "            wxb = weight.view(1, C_out, -1).repeat(N, 1, 1).matmul(unfolded)\n",
    "        #print(wxb.size())\n",
    "        return wxb.view(N, C_out, H_out, W_out)\n",
    "\n",
    "\n",
    "\n",
    "    def conv2d(input: Tensor, weight: Tensor, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor:\n",
    "        # input is 4d tensor\n",
    "        \n",
    "\n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride, stride)\n",
    "        if isinstance(padding, int):\n",
    "            padding = (padding, padding)\n",
    "        if isinstance(dilation, int):\n",
    "            dilation = (dilation, dilation)\n",
    "\n",
    "        N = input.size(0)\n",
    "        C_in = input.size(1)\n",
    "        H_in = input.size(-2)\n",
    "        W_in = input.size(-1)\n",
    "        \n",
    "        \n",
    "        kernel_size = (weight.size(-2), weight.size(-1))\n",
    "        C_out = weight.size(0)\n",
    "        H_out = int((H_in + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1)\n",
    "        W_out = int((W_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1)\n",
    "        \n",
    "        #weight = weight.repeat_interleave(groups,1)\n",
    "        \n",
    "        inp_unf = unfold(input,kernel,stride,padding,dilation)\n",
    "        print(\"weight\", weight.view(C_out, -1).t().size())\n",
    "        print(\"inp_\", inp_unf.transpose(1, 2).size())\n",
    "        if bias != None:\n",
    "            out_unf = inp_unf.transpose(1, 2).matmul(weight.view(C_out, -1).t()).transpose(1, 2).add(bias.view(1, -1, 1).repeat(N,1,H_out*W_out))\n",
    "        else:\n",
    "            out_unf = inp_unf.transpose(1, 2).matmul(weight.view(C_out, -1).t()).transpose(1, 2)\n",
    "            \n",
    "        out = fold(out_unf, (H_out,W_out), (1,1), dilation, padding, stride)\n",
    "        \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1185,
   "id": "5551fc23",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.empty'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c4/wb13541d4n104gj2c8s0pdf40000gn/T/ipykernel_1554/345974838.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munfold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch.empty'"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import fold, unfold\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class Conv2d(Module):\n",
    "    \n",
    "    def __init__(self,in_channel, out_channel, kernel_size = (2,2),stride=1, padding=0, dilation=1, groups=1,weight = None, bias = None):\n",
    "        if isinstance(stride, int):\n",
    "            self.stride = (stride, stride)\n",
    "        else:\n",
    "            self.stride = stride\n",
    "        if isinstance(padding, int):\n",
    "            self.padding = (padding, padding)\n",
    "        else: \n",
    "            self.padding = padding\n",
    "        if isinstance(dilation, int):\n",
    "            self.dilation = (dilation, dilation)\n",
    "        else:\n",
    "            self.dilation = dilation\n",
    "        if isinstance(kernel_size, int):\n",
    "            self.kernel = (kernel_size, kernel_size)\n",
    "        else:\n",
    "            self.kernel = kernel_size\n",
    "        \n",
    "        self.groups = groups\n",
    "        self.in_channel = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        \n",
    "        k = self.groups/(self.in_channel*self.kernel.prod())\n",
    "        if weight == None:\n",
    "            self.weight = torch.empty((self.out_channel,self.in_channel// self.groups,\n",
    "                                       self.kernel[0],self.kernel[1])).uniform_(-k,k)\n",
    "        else:\n",
    "            self.weight = weight\n",
    "        if bias == None:\n",
    "            self.bias = torch.empty(self.out_channel).uniform_(-k,k)\n",
    "        else:\n",
    "            self.bias = bias\n",
    "        \n",
    "        self.weight_grad = torch.empty((self.kernel[0],self.kernel[1]))\n",
    "        self.bias_grad = torch.empty(self.out_channel)\n",
    "        \n",
    "    @staticmethod\n",
    "    def conv2d(input: Tensor, weight: Tensor, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor:\n",
    "        # input is 4d tensor\n",
    "        \n",
    "\n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride, stride)\n",
    "        if isinstance(padding, int):\n",
    "            padding = (padding, padding)\n",
    "        if isinstance(dilation, int):\n",
    "            dilation = (dilation, dilation)\n",
    "\n",
    "        N = input.size(0)\n",
    "        C_in = input.size(1)\n",
    "        H_in = input.size(-2)\n",
    "        W_in = input.size(-1)\n",
    "        \n",
    "        \n",
    "        kernel_size = (weight.size(-2), weight.size(-1))\n",
    "        C_out = weight.size(0)\n",
    "        H_out = int((H_in + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1)\n",
    "        W_out = int((W_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1)\n",
    "        \n",
    "        inp_unf = unfold(input,kernel_size,dilation,padding,stride)\n",
    "        if bias != None:\n",
    "            out_unf = inp_unf.transpose(1, 2).matmul(weight.view(C_out, -1).t()).transpose(1, 2).add(bias.view(1, -1, 1).repeat(N,1,H_out*W_out))\n",
    "        else:\n",
    "            out_unf = inp_unf.transpose(1, 2).matmul(weight.view(C_out, -1).t()).transpose(1, 2)\n",
    "        return out_unf.view(N,C_out,H_out,W_out)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        \n",
    "        if (len(input.size()) == 4) :\n",
    "            return self.conv2d(self.input,weight = self.weight, bias = self.bias,stride =self.stride,\n",
    "                                  padding = self.padding, dilation = self.dilation,groups = self.groups)\n",
    "        \n",
    "    \n",
    "    def grad_conv2d_weight(self,input, weight_size, grad_output, stride=1, padding=0, dilation=1, groups=1):\n",
    "\n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride, stride)\n",
    "        if isinstance(padding, int):\n",
    "            padding = (padding, padding)\n",
    "        if isinstance(dilation, int):\n",
    "            dilation = (dilation, dilation)\n",
    "\n",
    "        N = input.size(0)\n",
    "        C_in = input.size(1)\n",
    "        H_in = input.size(-2)\n",
    "        W_in = input.size(-1)\n",
    "        \n",
    "        kernel_size = (weight_size[2], weight_size[3])\n",
    "        C_out = weight_size[0]\n",
    "        H_out = int((H_in + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1)\n",
    "        W_out = int((W_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1)\n",
    "\n",
    "        grad_w = torch.empty(weight_size)\n",
    "\n",
    "        for i in range(grad_w.size(0)): \n",
    "            for j in range(input.size(1)): \n",
    "                grad_w[i,j,:,:] = self.conv2d(self.input[:,j,:,:].view(N,1,H_in,W_in),\n",
    "                                              weight = grad_output[i,:,:].view(1,1,H_out,W_out), \n",
    "                                              bias = None,stride =self.dilation,padding = self.padding, \n",
    "                                              dilation = self.stride,groups = self.groups).narrow(2, 0, \n",
    "                                              weight_size[-2]).narrow(3, 0, weight_size[-1]).sum(dim=0)\n",
    "        \n",
    "        return grad_w\n",
    "        \n",
    "    def backward(self,gradwrtouput):\n",
    "        \n",
    "        a = self.grad_conv2d_weight(self.input, self.weight.shape, gradwrtouput, self.stride, self.padding, self.dilation, self.groups)\n",
    "        # the true bacward return the gradient with respect to the input\n",
    "        return a\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1179,
   "id": "57d4fa66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size torch.Size([1, 2, 5, 5])\n",
      "torch.Size([2, 4, 4, 4])\n",
      "torch.Size([1, 4, 8, 8]) torch.Size([2, 4, 4, 4]) torch.Size([2, 5, 5])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[4, 5, 4, 4]' is invalid for input of size 128",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c4/wb13541d4n104gj2c8s0pdf40000gn/T/ipykernel_1554/3991514678.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#grad_weight = torch.autograd.grad(output, filter, grad_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/grad.py\u001b[0m in \u001b[0;36mconv2d_weight\u001b[0;34m(input, weight_size, grad_output, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m    214\u001b[0m         grad_weight.shape[3])\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     return grad_weight.sum(dim=0).view(\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0min_channels\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mgrad_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[4, 5, 4, 4]' is invalid for input of size 128"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    input = torch.normal(mean = torch.zeros(1,4,8,8),std = 1)\n",
    "\n",
    "    kernel = torch.tensor([4,4])\n",
    "    stride = 1\n",
    "    padding = 0\n",
    "    dilation= 1\n",
    "    groups = 1\n",
    "\n",
    "    conv = torch.nn.Conv2d(4, 2, kernel, stride=stride, padding=padding, dilation=dilation, groups=groups)\n",
    "    weight = conv.state_dict()[\"weight\"]\n",
    "    bias = conv.state_dict()[\"bias\"]\n",
    "\n",
    "    conv_ = Conv2d(4,2,kernel,stride, padding, dilation, groups,weight, bias)\n",
    "\n",
    "    m = conv.forward(input) \n",
    "    print(\"size\",m.size())\n",
    "    m_ = conv_.forward(input)\n",
    "    #print(\"qwertz\")\n",
    "    #print(m_.size(),weight.size())\n",
    "    #m = torch.conv2d(input, weight, None, stride, padding,dilation, groups)\n",
    "    #print(torch.allclose(m,m_))\n",
    "    #print(torch.norm(m-m_))\n",
    "\n",
    "\n",
    "grad_output = torch.normal(mean=torch.zeros(m.size(1),m.size(-2),m.size(-1)),std= 1)\n",
    "with torch.no_grad():\n",
    "    grad = conv_.backward(grad_output)\n",
    "print(grad.size())\n",
    "\n",
    "input.requires_grad_(True)\n",
    "weight.requires_grad_(True)\n",
    "    \n",
    "#output = torch.nn.functional.conv2d(input, weight)\n",
    "#grad_weight = torch.autograd.grad(output, filter, grad_output)\n",
    "print(input.size(), weight.shape, grad_output.size())\n",
    "torch.nn.functional.grad.conv2d_weight(input, weight.shape, grad_output,stride = stride,dilation=dilation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1183,
   "id": "1f0b51fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ -0.8621,  -1.3539,  -1.4050,  -2.2326,  -1.5887,  -2.6613,  -0.9174,\n",
       "            -2.4680],\n",
       "          [  0.8768,   1.2882,  -3.1495,  -0.7159,  -2.7948,  -0.0425,  -6.4332,\n",
       "            -1.3337],\n",
       "          [  2.1938,   8.8172,   1.2869,   0.8178,  -2.6069,   3.5926,   1.4423,\n",
       "             3.4306],\n",
       "          [ -0.2965,   0.4006, -10.5096,  -2.9640,   0.5642,  -0.2168,   2.8331,\n",
       "            -2.2451],\n",
       "          [ -0.1506,   5.5349,   2.1488,   3.0071,   1.0212,  -4.3455,   1.9899,\n",
       "            -4.4598],\n",
       "          [ -1.7478,  -3.0436,  -3.1800,   4.5011,  -1.3357,  -0.3814,   5.8009,\n",
       "             1.9209],\n",
       "          [  0.2352,   1.3158,  -0.2542,  -1.8943,  -4.4092,   3.0154,   0.9633,\n",
       "             2.4462],\n",
       "          [ -0.3679,  -0.7948,   0.4415,   4.7018,   1.7258,  -1.8242,  -0.9733,\n",
       "             0.3006]],\n",
       "\n",
       "         [[ -1.7960,  -3.2907,   0.3881,  -1.8639,   2.9707,   1.9623,   1.9869,\n",
       "            -0.7144],\n",
       "          [  2.5084,   4.0274,   2.4384,  -1.5744,  -0.4300,  -2.5633,  -2.6203,\n",
       "            -0.1263],\n",
       "          [  1.2972,   4.9865,   4.6243,   2.7585,   5.0949,   3.1601,   1.2221,\n",
       "             1.0380],\n",
       "          [ -0.8138,  -0.6129,  -5.7223,  -6.4681,   4.0758,  -6.5341,   3.2453,\n",
       "            -0.5562],\n",
       "          [  1.2872,   9.2313,   8.9798,   3.4181,   3.9723,  -2.9156,   2.8134,\n",
       "            -0.5413],\n",
       "          [ -2.1945,  -5.7574,  -9.7967,   0.5651,   0.6597,  -1.1941,   1.3549,\n",
       "             0.4353],\n",
       "          [ -0.1263,   1.7490,   5.0981,   4.1475,  -6.0936,   1.5522,  -1.6572,\n",
       "             1.5142],\n",
       "          [ -0.4097,  -1.1540,  -1.2967,  -0.6915,   1.3550,   1.6119,  -0.2487,\n",
       "             0.9705]]]], grad_fn=<SlowConvTranspose2DBackward0>)"
      ]
     },
     "execution_count": 1183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(1,2,8,8, requires_grad=True)\n",
    "weight = torch.randn(2,2,4,4, requires_grad=True)\n",
    "output = torch.nn.functional.conv2d(input, weight)\n",
    "grad_output = torch.randn(output.shape)\n",
    "grad_input = torch.autograd.grad(output, input, grad_output)\n",
    "torch.nn.functional.grad.conv2d_input(input.shape, weight, grad_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "35dbac6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1064,  0.1317],\n",
       "         [ 0.1659, -0.0073],\n",
       "         [ 0.0467, -0.0940]],\n",
       "\n",
       "        [[-0.1064,  0.1317],\n",
       "         [ 0.1659, -0.0073],\n",
       "         [ 0.0467, -0.0940]],\n",
       "\n",
       "        [[-0.1078,  0.0370],\n",
       "         [-0.0660, -0.0301],\n",
       "         [ 0.0667,  0.0965]],\n",
       "\n",
       "        [[-0.1078,  0.0370],\n",
       "         [-0.0660, -0.0301],\n",
       "         [ 0.0667,  0.0965]],\n",
       "\n",
       "        [[ 0.0626,  0.0655],\n",
       "         [ 0.2033,  0.0281],\n",
       "         [ 0.1692, -0.0333]],\n",
       "\n",
       "        [[ 0.0626,  0.0655],\n",
       "         [ 0.2033,  0.0281],\n",
       "         [ 0.1692, -0.0333]],\n",
       "\n",
       "        [[-0.0681, -0.1378],\n",
       "         [ 0.0170, -0.0392],\n",
       "         [-0.1973,  0.0774]],\n",
       "\n",
       "        [[-0.0681, -0.1378],\n",
       "         [ 0.0170, -0.0392],\n",
       "         [-0.1973,  0.0774]]])"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.repeat_interleave(2,1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "5868996b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1064,  0.1317],\n",
       "         [ 0.1659, -0.0073],\n",
       "         [ 0.0467, -0.0940]],\n",
       "\n",
       "        [[-0.1078,  0.0370],\n",
       "         [-0.0660, -0.0301],\n",
       "         [ 0.0667,  0.0965]],\n",
       "\n",
       "        [[ 0.0626,  0.0655],\n",
       "         [ 0.2033,  0.0281],\n",
       "         [ 0.1692, -0.0333]],\n",
       "\n",
       "        [[-0.0681, -0.1378],\n",
       "         [ 0.0170, -0.0392],\n",
       "         [-0.1973,  0.0774]]])"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "d27ef7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 16, 49])\n",
      "torch.Size([2, 4, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])"
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.normal(mean = torch.zeros(30,4,8,8),std = 1)\n",
    "\n",
    "unfolded = unfold(input,kernel,stride,padding,dilation)\n",
    "print(unfolded.size())\n",
    "\n",
    "print(weight.size())\n",
    "w2 = weight.view(weight.size(0),-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "id": "454356da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 2, 42])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.6492e-06, grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 827,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.normal(mean = torch.zeros(30,4,8,8),std = 1)\n",
    "\n",
    "kernel = torch.tensor([3,2])\n",
    "stride = 1\n",
    "padding = 0\n",
    "dilation= 1\n",
    "groups = 1\n",
    "\n",
    "conv = torch.nn.Conv2d(4, 2, kernel, stride=stride, padding=padding, dilation=dilation, groups=groups)\n",
    "weight = conv.state_dict()[\"weight\"]\n",
    "bias = conv.state_dict()[\"bias\"]\n",
    "\n",
    "m = conv.forward(input) \n",
    "\n",
    "inp_unf = torch.nn.functional.unfold(input,kernel,stride,padding,dilation)\n",
    "out_unf = inp_unf.transpose(1, 2).matmul(weight.view(weight.size(0), -1).t()).transpose(1, 2).add(bias.view(1, -1, 1).repeat(30,1,inp_unf.size(2)))\n",
    "out = torch.nn.functional.fold(out_unf, (6,7), (1, 1),dilation,padding,stride)\n",
    "print(out_unf.size())\n",
    "(m - out).norm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "id": "48d146a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(input: Tensor, weight: Tensor, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor:\n",
    "        # input is 4d tensor\n",
    "        \n",
    "\n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride, stride)\n",
    "        if isinstance(padding, int):\n",
    "            padding = (padding, padding)\n",
    "        if isinstance(dilation, int):\n",
    "            dilation = (dilation, dilation)\n",
    "\n",
    "        N = input.size(0)\n",
    "        C_in = input.size(1)\n",
    "        H_in = input.size(-2)\n",
    "        W_in = input.size(-1)\n",
    "        \n",
    "        kernel_size = (weight.size(-2), weight.size(-1))\n",
    "        C_out = weight.size(0)\n",
    "        H_out = int((H_in + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1)\n",
    "        W_out = int((W_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1)\n",
    "        \n",
    "        weight = weight.repeat_interleave(groups,1)## pas aussi simple on ne somme plus sur toutes les in_channels maintenant\n",
    "        \n",
    "        inp_unf = unfold(input,kernel,stride,padding,dilation)#.view(N,H_out*W_out,-1,C_in//groups)\n",
    "        print(\"weight\", weight.view(C_out,-1, C_in//groups).size())\n",
    "        print(\"inp_\", inp_unf.size())\n",
    "        print(\"input\", input.size(),\"\\n\")\n",
    "        if bias != None:\n",
    "            \n",
    "            out_unf = inp_unf.transpose(1,2).matmul(weight.view(C_out,-1).t()).transpose(1,2).add(bias.view(1, -1, 1).repeat(N,1,H_out*W_out))\n",
    "        else:\n",
    "            out_unf = inp_unf.transpose(1, 2).matmul(weight.view(C_out, -1).t()).transpose(1, 2)\n",
    "            \n",
    "        out = torch.nn.functional.fold(out_unf, (H_out,W_out), (1,1), dilation, padding, stride)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "#out_unf = inp_unf.transpose(1, 2).matmul(weight.view(C_out, -1).t()).transpose(1, 2).add(bias.view(1, -1, 1).repeat(N,1,H_out*W_out))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 989,
   "id": "07916f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([31, 8, 8, 8]) torch.Size([4, 4, 3, 5]) None 1 0 1 2\n"
     ]
    }
   ],
   "source": [
    "input = torch.normal(mean = torch.zeros(31,8,8,8),std = 1)\n",
    "\n",
    "kernel = torch.tensor([3,5])\n",
    "stride = 1\n",
    "padding = 0\n",
    "dilation= 1\n",
    "groups = 2\n",
    "\n",
    "conv = torch.nn.Conv2d(8, 4, kernel, stride=stride, padding=padding, dilation=dilation, groups=groups)\n",
    "weight = conv.state_dict()[\"weight\"]\n",
    "bias = conv.state_dict()[\"bias\"]\n",
    "m = conv.forward(input)\n",
    "print(input.size(), weight.size(), None, dilation, padding,stride, groups)\n",
    "\n",
    "#m_ = conv2d(input, weight, bias, stride, padding, dilation, groups)\n",
    "m = torch.conv2d(input, weight, None, stride, padding,dilation, groups)\n",
    "#print(torch.allclose(m,m_))\n",
    "#print(torch.norm(m-m_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "id": "e65e1eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ho=0 wo=0\n",
      "x_seg=[[[ 0.18548936 -2.0822942   1.4365394   0.16685851]\n",
      "  [-0.5064098   0.56524706  1.0083085  -1.9693106 ]]\n",
      "\n",
      " [[ 0.05567319 -1.4576814   0.9803935   1.5677462 ]\n",
      "  [-1.2257185  -1.2196308  -1.5194861  -1.0492699 ]]\n",
      "\n",
      " [[ 0.07147467 -0.3886078  -0.2822727  -0.75422925]\n",
      "  [ 1.085428   -0.48508528  0.71002907  0.37662846]]\n",
      "\n",
      " [[-1.8497467   1.2027289   0.96429193 -1.7270403 ]\n",
      "  [ 0.5244083  -0.04580086 -0.7821546   0.0031531 ]]]\n",
      "dout=0.24331188201904297\n",
      "\n",
      "ho=0 wo=1\n",
      "x_seg=[[[-2.0822942   1.4365394   0.16685851 -0.5660924 ]\n",
      "  [ 0.56524706  1.0083085  -1.9693106   0.06564075]]\n",
      "\n",
      " [[-1.4576814   0.9803935   1.5677462  -1.045282  ]\n",
      "  [-1.2196308  -1.5194861  -1.0492699   1.0463142 ]]\n",
      "\n",
      " [[-0.3886078  -0.2822727  -0.75422925 -0.6288503 ]\n",
      "  [-0.48508528  0.71002907  0.37662846  1.9988246 ]]\n",
      "\n",
      " [[ 1.2027289   0.96429193 -1.7270403  -0.7148113 ]\n",
      "  [-0.04580086 -0.7821546   0.0031531   0.16711627]]]\n",
      "dout=-0.39551058411598206\n",
      "\n",
      "ho=0 wo=2\n",
      "x_seg=[[[ 1.4365394   0.16685851 -0.5660924  -1.6239557 ]\n",
      "  [ 1.0083085  -1.9693106   0.06564075 -0.32569015]]\n",
      "\n",
      " [[ 0.9803935   1.5677462  -1.045282    2.8192809 ]\n",
      "  [-1.5194861  -1.0492699   1.0463142   0.97802275]]\n",
      "\n",
      " [[-0.2822727  -0.75422925 -0.6288503   2.1028063 ]\n",
      "  [ 0.71002907  0.37662846  1.9988246   0.04097254]]\n",
      "\n",
      " [[ 0.96429193 -1.7270403  -0.7148113   0.42681158]\n",
      "  [-0.7821546   0.0031531   0.16711627 -0.85496044]]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 3 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c4/wb13541d4n104gj2c8s0pdf40000gn/T/ipykernel_1554/2736115587.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mback_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mback_w_naive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_backward_naive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/c4/wb13541d4n104gj2c8s0pdf40000gn/T/ipykernel_1554/422033621.py\u001b[0m in \u001b[0;36mconv_backward_naive\u001b[0;34m(dout, cache)\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0mx_seg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_pad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mho\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mho\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mHH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwo\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mwo\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mWW\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'x_seg={x_seg}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                     \u001b[0mdw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mx_seg\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'dout={dout[n, f, ho, wo]}\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 3 with size 2"
     ]
    }
   ],
   "source": [
    "input = torch.normal(mean = torch.zeros(30,4,8,8),std = 1)\n",
    "\n",
    "kernel = torch.tensor([2,4])\n",
    "stride = 1\n",
    "padding = 0\n",
    "dilation= 2\n",
    "groups = 1\n",
    "\n",
    "conv = torch.nn.Conv2d(4, 2, kernel, stride=stride, padding=padding, dilation=dilation, groups=groups)\n",
    "weight = conv.state_dict()[\"weight\"]\n",
    "bias = conv.state_dict()[\"bias\"]\n",
    "conv_ = Conv2d(4,2,kernel,stride, padding, dilation, groups,weight, bias)\n",
    "\n",
    "m = conv.forward(input) \n",
    "m_ = conv_.forward(input)\n",
    "\n",
    "dout = torch.normal(mean=torch.zeros(m.size(1),m.size(-2),m.size(-1)),std= 1)\n",
    "conv_param = {\"pad\": padding, \"stride\": stride}\n",
    "cache = (input.numpy(), weight.numpy(),bias.numpy(), conv_param)\n",
    "\n",
    "back_w = conv_.backward(dout)\n",
    "back_w_naive = conv_backward_naive(dout.view(1,dout.size(0),dout.size(1),dout.size(2)).repeat(30,1,1,1).numpy(),cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1158,
   "id": "be8ff886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def conv_backward_naive(dout, cache):\n",
    "    \"\"\"A naive implementation of the backward pass for a convolutional layer.\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives.\n",
    "    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x\n",
    "    - dw: Gradient with respect to w\n",
    "    - db: Gradient with respect to b\n",
    "    \"\"\"\n",
    "    dx, dw, db = None, None, None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the convolutional backward pass.                        #\n",
    "    ###########################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    # unpack the cache\n",
    "    x, w, b, conv_param = cache\n",
    "    N, C, H, W = x.shape\n",
    "    F, C, HH, WW = w.shape\n",
    "    s, pad = conv_param['stride'], conv_param['pad']\n",
    "    H_out = int(1 + (H + 2 * pad - HH) / s)\n",
    "    W_out = int(1 + (W + 2 * pad - WW) / s)\n",
    "\n",
    "    # pad x with 0s for H, W axis only\n",
    "    # (0, 0) means NO padding, (1, 1) - padding from both sides\n",
    "    x_pad = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)))\n",
    "    \n",
    "    # initialize gradients\n",
    "    dx = np.zeros_like(x)\n",
    "    dx_pad = np.zeros_like(x_pad)\n",
    "    dw = np.zeros_like(w)\n",
    "    db = np.zeros_like(b)\n",
    "\n",
    "    # iterate over examples in x\n",
    "    for n in range(N):\n",
    "        # iterate over filters in w\n",
    "        for f in range(F):\n",
    "            # iterate over x_pad with dout as a filter\n",
    "            for ho in range(H_out):\n",
    "                for wo in range(W_out):\n",
    "                    print(f'ho={ho} wo={wo}')\n",
    "                    # compute np.sum(x_seg * dout)\n",
    "                    x_seg = x_pad[n, :, ho*s:ho*s+HH, wo*s:wo*s+WW]\n",
    "                    print(f'x_seg={x_seg}')\n",
    "                    dw[f] += x_seg * dout[n, f, ho, wo] \n",
    "                    print(f'dout={dout[n, f, ho, wo]}\\n')\n",
    "\n",
    "    # remove padding\n",
    "    # dx[:, :, :, :] = dx_pad[:, :, pad:-pad, pad:-pad]\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return dx, dw, db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5707b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
